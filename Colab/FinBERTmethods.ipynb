{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune FinBERT to make predictions based on specific train and validation sets"
      ],
      "metadata": {
        "id": "AxQ4PrvRYkJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "class SequenceClassificationDataset(Dataset): # Handle the input data and labels for PyTorch's DataLoader\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs['input_ids']) # Return the total number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the input_ids, attention_mask, and label corresponding to the index\n",
        "        input_ids = self.inputs['input_ids'][idx]\n",
        "        attention_mask = self.inputs['attention_mask'][idx]\n",
        "        label = self.labels[idx]\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class FinBertFineTuning:\n",
        "    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer=None, device='cpu'):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.train_file = train_file\n",
        "        self.validation_file = validation_file\n",
        "        self.feature_col = feature_col\n",
        "        self.label_col = label_col\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.max_len = max_len\n",
        "        self.optimizer = optimizer\n",
        "        self.device = torch.device(device)  # Convert device argument to torch.device\n",
        "        drive.mount('/content/gdrive') # Mount Google Drive\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, max_len=self.max_len)\n",
        "\n",
        "        # Load datasets\n",
        "        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n",
        "        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n",
        "\n",
        "        # Calculate number of unique labels\n",
        "        self.num_labels = len(self.train_df[self.label_col].unique())\n",
        "\n",
        "        # Tokenize datasets\n",
        "        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n",
        "        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n",
        "\n",
        "        # Model configuration\n",
        "        self.model_config = AutoConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Optimizer\n",
        "        if self.optimizer is None:\n",
        "            raise ValueError(\"Please provide an optimizer instance.\")\n",
        "\n",
        "        if self.optimizer == 'Adam':\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        elif self.optimizer == 'AdamW':\n",
        "            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # DataLoaders\n",
        "        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n",
        "        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n",
        "\n",
        "    def tokenize_dataset(self, df, feature_col, label_col):\n",
        "        return self.tokenizer(list(df[feature_col]),\n",
        "                              padding=True,\n",
        "                              truncation=True,\n",
        "                              return_tensors='pt'), list(df[label_col])\n",
        "\n",
        "    def create_dataloader(self, tokenized_dataset, shuffle=True):\n",
        "        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n",
        "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n",
        "\n",
        "    def evaluate_model(self, dataloader):\n",
        "        self.model.eval() # Set the model to evaluation mode\n",
        "        # Initialize lists to store true labels and predictions\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader: # Iterate over batches in the data loader\n",
        "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
        "                labels = inputs[\"labels\"] # Extract labels from inputs\n",
        "                outputs = self.model(**inputs) # Forward pass through the model\n",
        "                logits = outputs.logits # Get logits from the model output\n",
        "\n",
        "                _, predicted = torch.max(logits, 1) # Compute predicted labels\n",
        "                # Convert labels and predictions to numpy arrays\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_predictions) # Calculate accuracy\n",
        "        return accuracy\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.num_epochs): # Iterate over the num_epochs of epochs\n",
        "            self.model.train() # Set the model to training mode\n",
        "            train_losses = [] # List to store training losses for each batch\n",
        "\n",
        "            # Iterate over batches in the training data loader, displaying progress using tqdm\n",
        "            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n",
        "                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
        "                outputs = self.model(**inputs) # Forward pass through the model\n",
        "                loss = outputs.loss # Retrieve the loss from the model output\n",
        "                train_losses.append(loss.item()) # Append the loss value to the list of training losses\n",
        "\n",
        "                self.optimizer.zero_grad() # Zero the gradients\n",
        "                loss.backward() # Backpropagate the gradients\n",
        "                self.optimizer.step() # Update the model parameters\n",
        "\n",
        "            # Validation\n",
        "            validation_losses = [] # Initialize an empty list to store validation losses\n",
        "            validation_accuracy = self.evaluate_model(self.validation_dataloader) # Evaluate model performance on the validation data loader\n",
        "\n",
        "            for batch in self.validation_dataloader:\n",
        "              inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n",
        "              outputs = self.model(**inputs) # Forward pass through the model\n",
        "              loss = outputs.loss # Retrieve the loss from the model output\n",
        "              validation_losses.append(loss.item()) # Append the loss value to the list of validation losses\n",
        "\n",
        "            print(f'Epoch {epoch + 1}/{self.num_epochs} - Training Loss: {sum(train_losses) / len(train_losses):.4f} - Validation Loss: {sum(validation_losses) / len(validation_losses):.4f} - Validation Accuracy: {validation_accuracy:.4f}')\n",
        "\n",
        "    def save_model(self, directory):\n",
        "        self.model.save_pretrained(directory)\n",
        "        self.tokenizer.save_pretrained(directory)\n",
        "\n",
        "# Usage\n",
        "start_time = time.time()\n",
        "model = 'finbert'\n",
        "model_name = 'ProsusAI/finbert'\n",
        "\n",
        "## Hyperparameters\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 3\n",
        "batch_size = 6\n",
        "\n",
        "# Maximum sequence length for padding and truncation\n",
        "max_len = 512\n",
        "\n",
        "optimizer = 'AdamW'  # Adam or AdamW\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "## Paths and filenames\n",
        "absolute_path = \"/content/gdrive/My Drive/CryptoNew/\"\n",
        "dataset_path = absolute_path + \"Datasets/\"\n",
        "train_file = 'train_set.csv'\n",
        "validation_file = 'validation_set.csv'\n",
        "feature_col = 'text'\n",
        "label_col = 'sentiment_numerical_fin'\n",
        "trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(\n",
        "    num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n",
        "\n",
        "# Fine-Tuning Phase\n",
        "classifier = FinBertFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size,\n",
        "                             learning_rate, num_epochs, max_len, optimizer, device)\n",
        "classifier.train()\n",
        "classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n",
        "print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "5dB2IPDz4NW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the Fine-tuned FinBERT model to make predictions for a specific test set"
      ],
      "metadata": {
        "id": "KBtolfD5Y3Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')  # Mount Google Drive\n",
        "\n",
        "absolute_path = \"/content/gdrive/My Drive/CryptoNew/\"\n",
        "test_file = 'test_set.csv'\n",
        "trained_model_name = 'finbert_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_512'\n",
        "\n",
        "test_df = pd.read_csv(os.path.join(absolute_path, 'Datasets', test_file))\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model_path = os.path.join(absolute_path, 'TrainedModels', trained_model_name)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise use CPU\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Tokenize test data\n",
        "tokenized_test = tokenizer(list(test_df['text']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inputs = {key: value.to(device) for key, value in tokenized_test.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "test_df['finbert_adam_ft_prediction'] = predicted_labels.cpu().numpy()\n",
        "\n",
        "# Save the test dataset with predictions\n",
        "test_df.to_csv(os.path.join(absolute_path, 'Datasets', 'test_set_1.csv'), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op_uIy1aGb_0",
        "outputId": "dc86a97f-7a24-4fab-f2e4-bf7132fca811"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}