{"cells":[{"cell_type":"markdown","metadata":{"id":"KeJ9KDW4L67C"},"source":["# Fine-tune BERT to make predictions based on specific train and validation sets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zoS8wgsLxts","executionInfo":{"status":"ok","timestamp":1716974539111,"user_tz":-180,"elapsed":75168,"user":{"displayName":"Alex Vandyke","userId":"04540880566492736626"}},"outputId":"804b2ced-a8f1-4c6a-c66b-bee470868382"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch 1/3: 100%|██████████| 534/534 [00:19<00:00, 26.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3 - Training Loss: 1.0148 - Validation Loss: 0.8182 - Validation Accuracy: 0.6412 - Validation Precision: 0.6401 - Validation Recall: 0.6412 - Validation F1 Score: 0.6399\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/3: 100%|██████████| 534/534 [00:19<00:00, 27.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/3 - Training Loss: 0.5355 - Validation Loss: 0.5185 - Validation Accuracy: 0.8225 - Validation Precision: 0.8274 - Validation Recall: 0.8225 - Validation F1 Score: 0.8210\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/3: 100%|██████████| 534/534 [00:19<00:00, 27.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3 - Training Loss: 0.2399 - Validation Loss: 0.4936 - Validation Accuracy: 0.8525 - Validation Precision: 0.8526 - Validation Recall: 0.8525 - Validation F1 Score: 0.8521\n","Training time: 74.94 seconds\n"]}],"source":["import os\n","import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import torch\n","import time\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","class SequenceClassificationDataset(Dataset): # Handle the input data and labels for PyTorch's DataLoader\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.inputs['input_ids']) # Return the total number of samples in the dataset\n","\n","    def __getitem__(self, idx):\n","        # Retrieve the input_ids, attention_mask, and label corresponding to the index\n","        input_ids = self.inputs['input_ids'][idx]\n","        attention_mask = self.inputs['attention_mask'][idx]\n","        label = self.labels[idx]\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","class BertFineTuning:\n","    def __init__(self, dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer=None, device='cpu'):\n","        self.dataset_path = dataset_path\n","        self.train_file = train_file\n","        self.validation_file = validation_file\n","        self.feature_col = feature_col\n","        self.label_col = label_col\n","        self.model_name = model_name\n","        self.batch_size = batch_size\n","        self.learning_rate = learning_rate\n","        self.num_epochs = num_epochs\n","        self.max_len = max_len\n","        self.optimizer = optimizer\n","        self.device = torch.device(device)  # Convert device argument to torch.device\n","        drive.mount('/content/gdrive') # Mount Google Drive\n","\n","        # Load tokenizer\n","        self.tokenizer = BertTokenizer.from_pretrained(self.model_name, max_len=self.max_len)\n","\n","        # Load datasets\n","        self.train_df = pd.read_csv(os.path.join(self.dataset_path, self.train_file))\n","        self.validation_df = pd.read_csv(os.path.join(self.dataset_path, self.validation_file))\n","\n","        # Calculate number of unique labels\n","        self.num_labels = len(self.train_df[self.label_col].unique())\n","\n","        # Tokenize datasets\n","        self.tokenized_train = self.tokenize_dataset(self.train_df, self.feature_col, self.label_col)\n","        self.tokenized_validation = self.tokenize_dataset(self.validation_df, self.feature_col, self.label_col)\n","\n","        # Model configuration\n","        self.model_config = BertConfig.from_pretrained(self.model_name, num_labels=self.num_labels)\n","        self.model = BertForSequenceClassification.from_pretrained(self.model_name, config=self.model_config)\n","        self.model.to(self.device)\n","\n","        # Optimizer\n","        if self.optimizer is None:\n","            raise ValueError(\"Please provide an optimizer instance.\")\n","\n","        if self.optimizer == 'Adam':\n","            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n","        elif self.optimizer == 'AdamW':\n","            self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n","\n","        # DataLoaders\n","        self.train_dataloader = self.create_dataloader(self.tokenized_train)\n","        self.validation_dataloader = self.create_dataloader(self.tokenized_validation, shuffle=False)\n","\n","    def tokenize_dataset(self, df, feature_col, label_col):\n","        return self.tokenizer(list(df[feature_col]),\n","                              padding=True,\n","                              truncation=True,\n","                              return_tensors='pt'), list(df[label_col])\n","\n","    def create_dataloader(self, tokenized_dataset, shuffle=True):\n","        dataset = SequenceClassificationDataset(tokenized_dataset[0], tokenized_dataset[1])\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle)\n","\n","    def evaluate_model(self, dataloader):\n","        self.model.eval() # Set the model to evaluation mode\n","        # Initialize lists to store true labels and predictions\n","        all_labels = []\n","        all_predictions = []\n","\n","        with torch.no_grad():\n","            for batch in dataloader: # Iterate over batches in the data loader\n","                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n","                labels = inputs[\"labels\"] # Extract labels from inputs\n","                outputs = self.model(**inputs) # Forward pass through the model\n","                logits = outputs.logits # Get logits from the model output\n","\n","                _, predicted = torch.max(logits, 1) # Compute predicted labels\n","                # Convert labels and predictions to numpy arrays\n","                all_labels.extend(labels.cpu().numpy())\n","                all_predictions.extend(predicted.cpu().numpy())\n","\n","        accuracy = accuracy_score(all_labels, all_predictions)\n","        precision = precision_score(all_labels, all_predictions, average='weighted')\n","        recall = recall_score(all_labels, all_predictions, average='weighted')\n","        f1 = f1_score(all_labels, all_predictions, average='weighted')\n","\n","        return accuracy, precision, recall, f1\n","\n","    def train(self):\n","        for epoch in range(self.num_epochs): # Iterate over the num_epochs of epochs\n","            self.model.train() # Set the model to training mode\n","            train_losses = [] # List to store training losses for each batch\n","\n","            # Iterate over batches in the training data loader, displaying progress using tqdm\n","            for batch in tqdm(self.train_dataloader, desc=f'Epoch {epoch + 1}/{self.num_epochs}'):\n","                inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n","                outputs = self.model(**inputs) # Forward pass through the model\n","                loss = outputs.loss # Retrieve the loss from the model output\n","                train_losses.append(loss.item()) # Append the loss value to the list of training losses\n","\n","                self.optimizer.zero_grad() # Zero the gradients\n","                loss.backward() # Backpropagate the gradients\n","                self.optimizer.step() # Update the model parameters\n","\n","            # Validation\n","            validation_losses = [] # Initialize an empty list to store validation losses\n","            validation_accuracy, validation_precision, validation_recall, validation_f1 = self.evaluate_model(self.validation_dataloader) # Evaluate model performance on the validation data loader\n","\n","            for batch in self.validation_dataloader:\n","              inputs = {key: value.to(self.device) for key, value in batch.items()} # Move inputs to the appropriate device (CPU or GPU)\n","              outputs = self.model(**inputs) # Forward pass through the model\n","              loss = outputs.loss # Retrieve the loss from the model output\n","              validation_losses.append(loss.item()) # Append the loss value to the list of validation losses\n","\n","            print(f'Epoch {epoch + 1}/{self.num_epochs} - Training Loss: {sum(train_losses) / len(train_losses):.4f} - Validation Loss: {sum(validation_losses) / len(validation_losses):.4f} - Validation Accuracy: {validation_accuracy:.4f} - Validation Precision: {validation_precision:.4f} - Validation Recall: {validation_recall:.4f} - Validation F1 Score: {validation_f1:.4f}')\n","\n","    def save_model(self, directory):\n","        self.model.save_pretrained(directory)\n","        self.tokenizer.save_pretrained(directory)\n","\n","# Usage\n","start_time = time.time()\n","model = 'bert'\n","model_name = 'bert-base-uncased'\n","\n","## Hyperparameters\n","learning_rate = 2e-5\n","num_epochs = 3\n","batch_size = 6\n","max_len = 512\n","\n","optimizer = 'Adam' # Adam or AdamW\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # or device = 'cpu'\n","\n","## Paths and filenames\n","absolute_path = \"/content/gdrive/My Drive/CryptoNew/\"\n","dataset_path = absolute_path + \"Datasets/\"\n","train_file = 'train_set.csv'\n","validation_file = 'validation_set.csv'\n","feature_col = 'text'\n","label_col = 'sentiment_numerical'\n","trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(num_epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n","\n","# Fine-Tuning Phase\n","classifier = BertFineTuning(dataset_path, train_file, validation_file, feature_col, label_col, model_name, batch_size, learning_rate, num_epochs, max_len, optimizer, device)\n","classifier.train()\n","classifier.save_model(absolute_path + 'TrainedModels/' + trained_model)\n","print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"]},{"cell_type":"markdown","metadata":{"id":"WkgyBo4QMF7N"},"source":["# Use the Fine-tuned BERT model to make predictions for a specific test set"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KXzJPPvDMILg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716974896382,"user_tz":-180,"elapsed":14750,"user":{"displayName":"Alex Vandyke","userId":"04540880566492736626"}},"outputId":"ed6f2175-a411-446a-d333-5abdbb8c11ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:10<00:00, 92.52it/s]\n"]}],"source":["import pandas as pd\n","import os\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","class BertPredictions:\n","    def __init__(self, model_path, device, max_len):\n","        drive.mount('/content/gdrive') # Mount Google Drive\n","        self.model_path = model_path\n","        self.max_len = max_len\n","        self.device = torch.device(device)  # Convert device argument to torch.device\n","        self.model, self.tokenizer = self.load_fine_tuned_bert_model()\n","\n","    def load_fine_tuned_bert_model(self):\n","        model = BertForSequenceClassification.from_pretrained(self.model_path) # Load the fine-tuned BERT model\n","        tokenizer = BertTokenizer.from_pretrained(self.model_path) # Load the tokenizer\n","        model.to(self.device) # Move the model to the specified device\n","        return model, tokenizer\n","\n","    def predict(self, input):\n","        tokens = self.tokenizer.tokenize(self.tokenizer.decode(self.tokenizer.encode(input))) # Tokenize the input using the loaded tokenizer\n","\n","        # Truncate the tokens if the length exceeds max_len - 2\n","        if len(tokens) > self.max_len - 2:\n","            tokens = tokens[:self.max_len - 2]\n","\n","        # Encode the tokens and convert them to PyTorch tensor\n","        input_ids = self.tokenizer.encode(tokens, return_tensors=\"pt\").to(self.device)\n","\n","        with torch.no_grad():\n","            self.model.eval() # Set the model to evaluation mode\n","            logits = self.model(input_ids)[0] # Perform forward pass through the model\n","            predictions = torch.argmax(logits, dim=1).item() # Predict the label by selecting the index with the highest logit value\n","\n","        return predictions\n","\n","    def predict_and_save(self, dataset_path, test_file, feature_col, prediction_col):\n","        # Load the test dataset\n","        test_df = pd.read_csv(os.path.join(dataset_path, test_file))\n","\n","        # Backup the original file by renaming it\n","        os.rename(os.path.join(dataset_path, test_file), os.path.join(dataset_path, 'test_set_original.csv'))\n","\n","        # Iterate through each row in the DataFrame\n","        for index, row in tqdm(test_df.iterrows(), total=len(test_df)):\n","            content = row[feature_col]\n","\n","            # Process the content and predict the label\n","            predicted_rating = self.predict(content)\n","\n","            # Update the prediction_col column\n","            test_df.at[index, prediction_col] = predicted_rating\n","\n","        # Save results to CSV\n","        test_df.to_csv(os.path.join(dataset_path, test_file), index=False)\n","\n","# Usage\n","max_len = 512\n","\n","str_params = 'bert_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_512'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Determine device\n","optimizer = \"Adam\"  # Set the correct optimizer\n","\n","## Paths and filenames\n","path = \"/content/gdrive/My Drive/CryptoNew/\"\n","dataset_path = path + \"Datasets/\"\n","test_file = \"test_set.csv\"\n","trained_model = path + 'TrainedModels/' + str_params  # The fine-tuned model\n","feature_col = 'text'\n","# prediction_col = str_params + '_prediction'\n","prediction_col = 'bert_adam_ft_prediction_new2'\n","\n","# Instantiate the BertPredictions class\n","prediction = BertPredictions(trained_model, device, max_len)\n","\n","# Run prediction and save results to CSV\n","prediction.predict_and_save(dataset_path, test_file, feature_col, prediction_col)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}